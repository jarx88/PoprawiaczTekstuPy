# from .base_client import APIConnectionError, APIResponseError, DEFAULT_TIMEOUT # Zakomentowano - nieu≈ºywane w tym pliku
import time # Dla symulacji op√≥≈∫nienia
import openai
import os
import sys # Dodano import sys
from datetime import datetime
import ssl
import httpx
from httpx import HTTPError, TimeoutException
# PyQt6 removed - using CustomTkinter GUI now
from gui.prompts import get_system_prompt
from .base_client import DEFAULT_TIMEOUT, QUICK_TIMEOUT, CONNECTION_TIMEOUT, DEFAULT_RETRIES, APITimeoutError

# Importujemy logger z odpowiedniego miejsca w strukturze projektu
# Zak≈Çadamy, ≈ºe api_clients jest na tym samym poziomie co utils
# wiƒôc potrzebujemy .utils.logger
try:
    from utils.logger import logger
    from utils.config_manager import get_config_value
except ImportError:
    # Fallback logger w przypadku problem√≥w z importem (np. bezpo≈õrednie uruchamianie)
    import logging
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO) # Domy≈õlny poziom dla fallbacka
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

# Klucz API OpenAI jest teraz przekazywany jako argument do funkcji,
# ale mo≈ºna te≈º ustawiƒá go globalnie przez zmiennƒÖ ≈õrodowiskowƒÖ OPENAI_API_KEY
# Je≈õli chcesz u≈ºywaƒá zmiennej ≈õrodowiskowej, odkomentuj:
# openai.api_key = os.getenv("OPENAI_API_KEY")

# Tymczasowo usuwamy import relatywny, je≈õli bƒôdziemy go potrzebowaƒá p√≥≈∫niej, przywr√≥cimy
# from .base_client import APIConnectionError, APIResponseError, DEFAULT_TIMEOUT

def show_connection_error():
    """Log connection error - GUI now handled by main application"""
    logger.error("Connection error - cannot connect to API server")
    logger.error("Possible causes: 1) No internet 2) Firewall blocking 3) API server down")

def handle_api_error(e):
    if isinstance(e, (httpx.ConnectError, httpx.ConnectTimeout, openai.APIConnectionError)):
        logger.error(f"B≈ÇƒÖd po≈ÇƒÖczenia: {str(e)}")
        show_connection_error()
        return True
    return False

_OPENAI_CLIENT_CACHE = {}
_HTTP2_AVAILABLE = True
try:
    import h2  # type: ignore
except Exception:
    _HTTP2_AVAILABLE = False


def _get_openai_client(api_key: str) -> openai.OpenAI:
    """Zwraca cache'owanego klienta OpenAI z HTTP/2 i po≈ÇƒÖczeniami keep-alive."""
    client = _OPENAI_CLIENT_CACHE.get(api_key)
    if client is not None:
        return client

    http_client = httpx.Client(
        http2=_HTTP2_AVAILABLE,
        timeout=httpx.Timeout(
            connect=CONNECTION_TIMEOUT,
            read=DEFAULT_TIMEOUT,
            write=CONNECTION_TIMEOUT,
            pool=CONNECTION_TIMEOUT,
        ),
        limits=httpx.Limits(max_keepalive_connections=10, keepalive_expiry=30.0),
    )

    client = openai.OpenAI(
        api_key=api_key,
        timeout=httpx.Timeout(
            connect=CONNECTION_TIMEOUT,
            read=DEFAULT_TIMEOUT,
            write=CONNECTION_TIMEOUT,
            pool=CONNECTION_TIMEOUT,
        ),
        max_retries=DEFAULT_RETRIES,
        http_client=http_client,
    )
    _OPENAI_CLIENT_CACHE[api_key] = client
    return client


def correct_text_openai(api_key, model, text_to_correct, instruction_prompt, system_prompt, on_chunk=None):
    """Poprawia tekst u≈ºywajƒÖc OpenAI API."""
    
        
    if not api_key:
        logger.warning("Pr√≥ba u≈ºycia OpenAI API bez klucza.") # Logowanie ostrze≈ºenia
        return "B≈ÇƒÖd: Klucz API OpenAI nie zosta≈Ç podany."
    if not model:
        logger.warning("Pr√≥ba u≈ºycia OpenAI API bez podania modelu.") # Logowanie ostrze≈ºenia
        return "B≈ÇƒÖd: Model OpenAI nie zosta≈Ç okre≈õlony."
    if not text_to_correct:
        logger.warning("Pr√≥ba u≈ºycia OpenAI API bez tekstu do poprawy.") # Logowanie ostrze≈ºenia
        return "B≈ÇƒÖd: Brak tekstu do poprawy."

    logger.info(f"Wysy≈Çanie zapytania do OpenAI API (model: {model}). Tekst: {text_to_correct[:50]}...") # Logowanie rozpoczƒôcia zapytania

    try:
        # Klient OpenAI ‚Äì HTTP/2 oraz keep-alive (cache per api_key)
        client = _get_openai_client(api_key)

        # U≈ºyj przekazanego system_prompt; je≈õli pusty, wybierz wg stylu
        if not system_prompt:
            current_system_prompt = get_system_prompt("prompt" if "prompt" in instruction_prompt.lower() else "normal")
        else:
            current_system_prompt = system_prompt
        
        # Przygotowanie wiadomo≈õci dla Chat Completions API
        messages = [
            {"role": "system", "content": current_system_prompt},
            {"role": "user", "content": f"{instruction_prompt}\n\n---\n{text_to_correct}\n---"}
        ]

        # Wys≈Çanie zapytania do API z timeout
        response = None
        
        logger.info(f"üîç DEBUG: Rozpoczynam korekcjƒô dla modelu: {model}")

        # GPT-5 i o1 modele WYMAGAJƒÑ Responses API (nie dzia≈ÇajƒÖ z Chat Completions)
        # gpt-4o-mini u≈ºywa Chat Completions API
        use_responses_api = any(model.lower().startswith(prefix) for prefix in ["gpt-5", "o1"])
        
        logger.info(f"üîç DEBUG: Model: {model}, use_responses_api: {use_responses_api}")
        
        try:
            if use_responses_api:
                # Responses API dla nowszych modeli z reasoning controls
                # Pobierz ustawienia z config.ini lub u≈ºyj domy≈õlnych
                try:
                    import configparser
                    from utils.config_manager import get_config_path
                    config = configparser.ConfigParser()
                    config.read(get_config_path())
                    reasoning_effort = get_config_value(config, "AI_SETTINGS", "ReasoningEffort", "high")
                    verbosity = get_config_value(config, "AI_SETTINGS", "Verbosity", "medium")
                except:
                    # Fallback values je≈õli problem z konfiguracjƒÖ
                    reasoning_effort = "high"
                    verbosity = "medium"
                
                logger.info(f"OpenAI Responses API: model={model}, reasoning_effort={reasoning_effort}, verbosity={verbosity}")
                
                # Sprawd≈∫ czy SDK ma responses API
                if not hasattr(client, 'responses'):
                    logger.warning(f"SDK brak responses API - fallback do chat completions dla {model}")
                    raise AttributeError("No responses API in SDK")
                # PRAWID≈ÅOWA sk≈Çadnia Responses API z dokumentacji OpenAI 2025
                # Test r√≥≈ºnych format√≥w nazwy modelu
                model_variants = [model]
                if model == "gpt-5-mini":
                    model_variants = ["gpt-5-mini", "gpt-5-mini-2025-08-07", "gpt-5-mini-preview", "o1-mini", "gpt-4o-mini"]
                elif model == "gpt-5":
                    model_variants = ["gpt-5", "gpt-5-2025-08-07", "gpt-5-preview", "o1-preview", "gpt-4o"]
                elif model == "o1-mini":
                    model_variants = ["o1-mini", "gpt-4o-mini"]
                
                response = None
                last_error = None
                corrected_text = ""

                # 1) Je≈õli chcemy stream i SDK/plan wspiera Responses.stream ‚Äì spr√≥buj najpierw STREAM (simple payload)
                if callable(on_chunk):
                    for variant in model_variants:
                        try:
                            stream_ctx = client.responses.stream(
                                model=variant,
                                input=f"{current_system_prompt}\n\n{instruction_prompt}\n\n---\n{text_to_correct}\n---",
                                max_output_tokens=2000,
                            )
                            collected = []
                            with stream_ctx as stream:
                                for event in stream:
                                    if getattr(event, 'type', '') == 'response.output_text.delta':
                                        delta_text = getattr(event, 'delta', '') or ''
                                        if delta_text:
                                            collected.append(delta_text)
                                            try:
                                                on_chunk(delta_text)
                                            except Exception:
                                                pass
                                try:
                                    final = stream.get_final_response()
                                except Exception:
                                    final = None
                            if final is not None and hasattr(final, 'output_text') and final.output_text:
                                corrected_text = final.output_text.strip()
                            else:
                                corrected_text = ("".join(collected)).strip()
                            logger.info(f"‚úÖ OpenAI Responses STREAM ok (variant={variant}), len={len(corrected_text)}")
                            break
                        except Exception as e_stream:
                            logger.warning(f"Responses.stream failed for {variant}: {e_stream}")
                            last_error = e_stream
                            corrected_text = ""
                            continue
                    # Je≈õli stream siƒô uda≈Ç ‚Äì pomi≈Ñ create()
                    if corrected_text:
                        # Przejd≈∫ do sekcji ko≈Ñcowego przetworzenia odpowiedzi poni≈ºej
                        pass
                    else:
                        logger.info("Responses stream niedostƒôpny/nieudany ‚Äì fallback do create()")

                # 2) Non-stream create() z pr√≥bƒÖ rich‚Üísimple payload
                if not corrected_text:
                    for variant in model_variants:
                        logger.info(f"Pr√≥bujƒô model variant: {variant}")
                        # Dwie pr√≥by: 1) z parametrami reasoning/text, 2) bez tych p√≥l
                        attempt_payloads = [
                        {
                            "model": variant,
                            "input": f"{current_system_prompt}\n\n{instruction_prompt}\n\n---\n{text_to_correct}\n---",
                            "reasoning": {"effort": reasoning_effort},
                            "text": {"verbosity": verbosity},
                            "max_output_tokens": 2000,
                        },
                        {
                            "model": variant,
                            "input": f"{current_system_prompt}\n\n{instruction_prompt}\n\n---\n{text_to_correct}\n---",
                            "max_output_tokens": 2000,
                        },
                        ]

                        for payload in attempt_payloads:
                            try:
                                # Je≈õli poprzedni b≈ÇƒÖd dotyczy≈Ç unsupported_parameter, przejd≈∫ od razu do uproszczonego payloadu
                                if last_error and ("unsupported_parameter" in str(last_error).lower() or "Unsupported parameter" in str(last_error)):
                                    if "reasoning" in payload or "text" in payload:
                                        continue
                                response = client.responses.create(**payload)
                                logger.info(f"‚úÖ Sukces z modelem: {variant} (payload: {'simple' if 'reasoning' not in payload else 'rich'})")
                                break
                            except Exception as e2:
                                logger.warning(f"Variant {variant} attempt failed: {e2}")
                                last_error = e2
                                response = None
                                continue
                        if response is not None:
                            break
                
                if not corrected_text:
                    if response is None:
                        raise last_error or Exception("All model variants failed")
                    # Responses API: preferuj output_text je≈õli dostƒôpny, bez sklejania duplikat√≥w
                    logger.info(f"Responses API response type: {type(response)}")
                    logger.info(f"Response hasattr output: {hasattr(response, 'output')}")
                    logger.info(f"Response hasattr content: {hasattr(response, 'content')}")

                    # PRAWID≈ÅOWE parsowanie Responses API - JEDEN ≈∫r√≥d≈Ço tekstu
                    # Sprawd≈∫ wszystkie mo≈ºliwe atrybuty i u≈ºyj TYLKO PIERWSZEGO znalezionego
                    if hasattr(response, 'output_text') and response.output_text:
                        corrected_text = response.output_text.strip()
                        logger.info(f"‚úÖ Got output_text: {len(corrected_text)} chars")
                    elif hasattr(response, 'response') and response.response:
                        corrected_text = response.response.strip()
                        logger.info(f"‚úÖ Got response field: {len(corrected_text)} chars")
                    elif hasattr(response, 'content') and response.content:
                        corrected_text = str(response.content).strip()
                        logger.info(f"‚úÖ Got content field: {len(corrected_text)} chars")
                    else:
                        logger.warning("‚ùå No recognizable field in Responses API")
                        attrs = [attr for attr in dir(response) if not attr.startswith('_')]
                        logger.info(f"Available attributes: {attrs}")
                        corrected_text = str(response).strip() if response else ""
                    logger.info(f"Extracted text length: {len(corrected_text)} chars")
            else:
                logger.info(f"üîç DEBUG: U≈ºywam Chat Completions API dla modelu: {model}")
                if callable(on_chunk):
                    # Streaming delta
                    stream = client.chat.completions.create(
                        model=model,
                        messages=messages,
                        stream=True,
                        max_tokens=2000
                    )
                    collected = []
                    try:
                        for chunk in stream:
                            try:
                                delta = chunk.choices[0].delta.content or ''
                            except Exception:
                                delta = ''
                            if delta:
                                collected.append(delta)
                                try:
                                    on_chunk(delta)
                                except Exception:
                                    pass
                    except Exception as e:
                        logger.warning(f"OpenAI stream interrupted: {e}")
                    corrected_text = ("".join(collected)).strip()
                else:
                    response = client.chat.completions.create(
                        model=model,
                        messages=messages,
                        max_tokens=2000
                    )
                    logger.info(f"üîç DEBUG: Chat Completions response received")
                    # Chat Completions API
                    if response.choices and response.choices[0].message:
                        corrected_text = (response.choices[0].message.content or '').strip()
                        logger.info(f"üîç DEBUG: Extracted from choices[0].message.content: {len(corrected_text)} chars")
                    else:
                        corrected_text = ""
                        logger.warning(f"üîç DEBUG: No choices or message in response")
        except (AttributeError, TypeError, Exception) as e:
            # GPT-5 modele dzia≈ÇajƒÖ TYLKO z Responses API - nie pr√≥buj fallback do Chat Completions
            if use_responses_api and any(model.lower().startswith(prefix) for prefix in ["gpt-5", "o1"]):
                logger.error(f"GPT-5 model {model} failed with Responses API: {type(e).__name__}: {e}")
                # Mo≈ºliwe przyczyny: b≈Çƒôdna nazwa modelu, brak dostƒôpu, stary SDK
                if "404" in str(e) or "not found" in str(e).lower():
                    return f"B≈ÇƒÖd: Model {model} nie zosta≈Ç znaleziony. Mo≈ºliwe nazwy: gpt-5-mini, gpt-5-nano, gpt-5. Sprawd≈∫ dostƒôp do GPT-5 models w OpenAI account."
                elif "authentication" in str(e).lower():
                    return f"B≈ÇƒÖd: Brak autoryzacji dla {model}. Sprawd≈∫ klucz API i dostƒôp do GPT-5 models."
                else:
                    return f"B≈ÇƒÖd GPT-5 Responses API: {e}. Sprawd≈∫ SDK version: pip install openai --upgrade"
            
            # Fallback: SDK nie ma responses API, model nie wspiera parametr√≥w reasoning, lub inne b≈Çƒôdy API
            logger.warning(f"Responses API fallback dla {model}: {type(e).__name__}: {e}")
            
            # Pr√≥buj standardowe Chat Completions API (tylko dla nie-GPT-5 modeli)
            try:
                if callable(on_chunk):
                    stream = client.chat.completions.create(
                        model=model,
                        messages=messages,
                        stream=True,
                        max_tokens=2000
                    )
                    collected = []
                    try:
                        for chunk in stream:
                            try:
                                delta = chunk.choices[0].delta.content or ''
                            except Exception:
                                delta = ''
                            if delta:
                                collected.append(delta)
                                try:
                                    on_chunk(delta)
                                except Exception:
                                    pass
                    except Exception as e:
                        logger.warning(f"OpenAI stream (fallback) interrupted: {e}")
                    corrected_text = ("".join(collected)).strip()
                else:
                    response = client.chat.completions.create(
                        model=model,
                        messages=messages,
                        max_tokens=2000
                    )
                    corrected_text = (response.choices[0].message.content or '').strip() if (response.choices and response.choices[0].message) else ""
                logger.info(f"Chat Completions API fallback successful, text length: {len(corrected_text)} chars")
            except Exception as fallback_error:
                logger.error(f"Both Responses and Chat Completions API failed for {model}: {fallback_error}")
                # Sprawd≈∫ typowe liter√≥wki w nazwie modelu
                if "gtp-5" in model.lower():
                    suggested_model = model.replace("gtp-5", "gpt-5")
                    return f"B≈ÇƒÖd: Model {model} niedostƒôpny. Czy chodzi≈Ço o '{suggested_model}'? Popraw nazwƒô modelu w ustawieniach."
                return f"B≈ÇƒÖd: Model {model} niedostƒôpny. Sprawd≈∫ nazwƒô modelu lub spr√≥buj gpt-4o-mini."

        # Przetworzenie odpowiedzi
        logger.info(f"üîç DEBUG: corrected_text d≈Çugo≈õƒá: {len(corrected_text) if corrected_text else 'None'}")
        logger.info(f"üîç DEBUG: corrected_text content (50 chars): {corrected_text[:50] if corrected_text else 'EMPTY'}")
        
        if corrected_text:
            logger.info("‚úÖ Otrzymano poprawnƒÖ odpowied≈∫ od OpenAI API.")
            logger.info(f"üîç DEBUG: Original response (100 chars): '{corrected_text[:100]}...'")
            
            # Czyszczenie odpowiedzi - bardziej ostro≈ºne
            original_text = corrected_text
            corrected_text = corrected_text.strip()
            logger.info(f"üîç DEBUG: Po strip: {len(corrected_text)} chars")
            
            # Usu≈Ñ wszystkie wystƒÖpienia --- z poczƒÖtku i ko≈Ñca (ale zachowaj tre≈õƒá)
            while corrected_text.startswith("---"):
                corrected_text = corrected_text[3:].strip()
                logger.info(f"üîç DEBUG: Po usuwaniu --- z poczƒÖtku: {len(corrected_text)} chars")
            while corrected_text.endswith("---"):
                corrected_text = corrected_text[:-3].strip()
                logger.info(f"üîç DEBUG: Po usuwaniu --- z ko≈Ñca: {len(corrected_text)} chars")
            
            # Dodatkowe czyszczenie - usu≈Ñ linie zawierajƒÖce same ---
            lines_before = corrected_text.splitlines()
            lines = [line for line in lines_before if line.strip() != "---"]
            logger.info(f"üîç DEBUG: Linie przed: {len(lines_before)}, po usuniƒôciu ---: {len(lines)}")
            corrected_text = "\n".join(lines).strip()
            
            # Usu≈Ñ puste linie na poczƒÖtku i ko≈Ñcu (ale zostaw niepuste)
            lines = [line for line in corrected_text.splitlines() if line.strip()]
            logger.info(f"üîç DEBUG: Po usuniƒôciu pustych linii: {len(lines)} linii")
            
            # Usu≈Ñ pierwszƒÖ liniƒô je≈õli to nazwa stylu
            style_names = ["normal", "professional", "translate_en", "translate_pl", "change_meaning", "summary"]
            if lines and any(style in lines[0].lower() for style in style_names):
                logger.info(f"üîç DEBUG: Usuwam pierwszƒÖ liniƒô (style): '{lines[0]}'")
                lines = lines[1:]
            
            final_result = "\n".join(lines).strip()
            logger.info(f"üîç DEBUG: Final result: {len(final_result)} chars: '{final_result[:100]}...'")
            
            # Proste przekazanie bez deduplikacji - pozwolƒô Responses API zwr√≥ciƒá co chce
            
            # Je≈õli po czyszczeniu nic nie zosta≈Ço, zwr√≥ƒá original
            if not final_result and original_text:
                logger.warning(f"‚ùå Czyszczenie usunƒô≈Ço ca≈ÇƒÖ tre≈õƒá! Zwracam oryginalnƒÖ odpowied≈∫")
                return original_text.strip()
            
            return final_result
        else:
            logger.warning("Otrzymano odpowied≈∫ od OpenAI, ale tre≈õƒá jest pusta.")
            return "B≈ÇƒÖd: Nie otrzymano poprawnej odpowiedzi od OpenAI API (brak tre≈õci w wiadomo≈õci)."

    except (httpx.TimeoutException, httpx.ReadTimeout, httpx.ConnectTimeout) as e:
        logger.error(f"Timeout OpenAI API: {e}", exc_info=True)
        return f"B≈ÇƒÖd: OpenAI API nie odpowiada (timeout {DEFAULT_TIMEOUT}s). Spr√≥buj ponownie."
    except (HTTPError, TimeoutException, openai.APIConnectionError) as e:
        if handle_api_error(e):
            return "B≈ÇƒÖd po≈ÇƒÖczenia z API. Sprawd≈∫ komunikat b≈Çƒôdu."
        logger.error(f"B≈ÇƒÖd po≈ÇƒÖczenia z OpenAI API: {e}", exc_info=True) # Logowanie b≈Çƒôdu
        return f"B≈ÇƒÖd po≈ÇƒÖczenia z OpenAI API: {e}"
    except openai.RateLimitError as e:
        logger.warning(f"Przekroczono limit zapyta≈Ñ do OpenAI API: {e}") # Logowanie ostrze≈ºenia
        return f"B≈ÇƒÖd OpenAI (limit zapyta≈Ñ): {e}"
    except openai.AuthenticationError as e:
        logger.error(f"B≈ÇƒÖd autentykacji OpenAI API (prawdopodobnie z≈Çy klucz): {e}", exc_info=True) # Logowanie b≈Çƒôdu
        return f"B≈ÇƒÖd OpenAI (autentykacja - z≈Çy klucz?): {e}"
    except openai.APIStatusError as e:
        logger.error(f"Og√≥lny b≈ÇƒÖd statusu OpenAI API: {e} (Status: {e.status_code}, Response: {e.response})", exc_info=True) # Logowanie b≈Çƒôdu z dodatkowymi informacjami
        return f"B≈ÇƒÖd OpenAI (status API {e.status_code}): {e.response}"
    except Exception as e:
        logger.error(f"Nieoczekiwany b≈ÇƒÖd podczas komunikacji z OpenAI: {e}", exc_info=True) # Logowanie b≈Çƒôdu
        return f"B≈ÇƒÖd OpenAI (nieoczekiwany): {e}"


if __name__ == '__main__':
    # Prosty test dzia≈Çania (wymaga ustawienia klucza API i modelu poni≈ºej lub w zmiennych ≈õrodowiskowych)
    # Pamiƒôtaj, aby zastƒÖpiƒá 'YOUR_OPENAI_API_KEY' i 'YOUR_MODEL' (np. "gpt-3.5-turbo")
    # lub za≈Çadowaƒá je z pliku konfiguracyjnego.

    # --- Modyfikacja sys.path dla testowania bezpo≈õredniego ---
    # Zak≈Çadamy, ≈ºe ten plik (openai_client.py) jest w PoprawiaczTekstuPy/api_clients/
    # Musimy dodaƒá PoprawiaczTekstuPy do sys.path, aby importy z api_clients zadzia≈Ça≈Çy
    current_script_path = os.path.abspath(__file__)
    # api_clients_dir to .../PoprawiaczTekstuPy/api_clients
    api_clients_dir = os.path.dirname(current_script_path)
    # project_root_dir to .../PoprawiaczTekstuPy
    project_root_dir = os.path.dirname(api_clients_dir)
    
    if project_root_dir not in sys.path:
        sys.path.insert(0, project_root_dir)
    
    # Teraz mo≈ºemy spr√≥bowaƒá importu, kt√≥ry wcze≈õniej zawodzi≈Ç, je≈õli by≈Ç potrzebny
    # from api_clients.base_client import APIConnectionError, APIResponseError # Przyk≈Çad
    # W tym konkretnym pliku openai_client.py nie u≈ºywamy niczego z base_client,
    # wiƒôc linia 'from .base_client import...' na g√≥rze mo≈ºe zostaƒá usuniƒôta lub zakomentowana
    # je≈õli nie planujemy jej u≈ºywaƒá.
    # Dla czysto≈õci, je≈õli nie jest u≈ºywana, lepiej jƒÖ usunƒÖƒá.
    # Na potrzeby tego testu, zak≈Çadamy, ≈ºe nie jest potrzebna w tym pliku.

    # --- PRZYK≈ÅAD U≈ªYCIA --- 
    # Ustaw poni≈ºsze zmienne przed uruchomieniem tego bloku testowego
    test_api_key = "YOUR_DEEPSEEK_API_KEY"  # Wstaw sw√≥j klucz OpenAI
    test_model = "o4-mini"      # Wstaw model, np. "gpt-3.5-turbo"
    
    if test_api_key == "YOUR_OPENAI_API_KEY" or not test_api_key:
        print("Aby przetestowaƒá, ustaw `test_api_key` oraz `test_model` w sekcji if __name__ == '__main__'.")
    else:
        sample_text = "To jest tekst z b≈Çendem ortograficznym i gramatycznym. Popraw go proszƒô."
        sample_instruction = "Popraw nastƒôpujƒÖcy tekst, zachowujƒÖc jego formatowanie."
        sample_system_prompt = (
            'You are a virtual editor specializing in proofreading Polish texts. '
            'Your goal is to transform the provided text into correct, clear, and professional-sounding Polish, '
            'eliminating all language errors. The input text will be in Polish. Instructions: '
            '1. **Error-Free**: Detect and correct ALL spelling, grammatical, punctuation, and stylistic errors. Focus on precision and compliance with Polish language standards. '
            '2. **Clarity and Conciseness**: Simplify complex sentences while preserving their technical meaning. Aim for clear and precise communication. Eliminate redundant words and repetitions. '
            '3. **IT Terminology**: Preserve original technical terms, proper names, acronyms, and code snippets, unless they contain obvious spelling mistakes. Do not change their meaning. '
            '4. **Professional Tone**: Give the text a professional yet natural tone. Avoid colloquialisms, but also excessive formality. '
            '5. **Formatting**: Strictly preserve the original text formatting: paragraphs, bulleted/numbered lists, indentations, bolding (if Markdown was used), and line breaks. '
            '6. **Text Only**: As the result, return ONLY the final, corrected Polish text, without any additional comments, headers, or explanations.'
        )
        
        print(f"Wysy≈Çanie zapytania do OpenAI z modelem: {test_model}...")
        result = correct_text_openai(test_api_key, test_model, sample_text, sample_instruction, sample_system_prompt)
        print("--- Wynik z OpenAI ---")
        print(result)
        print("----------------------") 